{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Use Tensorboard\n",
    "-------------\n",
    "\n",
    "The two main advantages of TensorFlow over many other available libraries are **flexibility** and **visualization**  \n",
    "\n",
    "Imagine if you can visualize whats happening in the code (in this case code represents the computational graph that we create for a model), it would be so convenient to deeply understand and observe the inner workings of the graph. Not just that, it also helps in fixing things that are not working the way they should. TensorFlow provides a way to do just that using **TensorBoard!**\n",
    "\n",
    "------------------\n",
    "### What is Tensorboard?\n",
    "\n",
    "- TensorBoard is a visualization software that comes with any standard TensorFlow installation    \n",
    "      \n",
    "      \n",
    "    In Google’s words:\n",
    "        \n",
    "             |  “ The computations you’ll use TensorFlow for many things (like training a massive deep neural network)\n",
    "             |    and they can be complex and confusing. To make it easier to understand, debug, and optimize \n",
    "             |    TensorFlow programs, we’ve included a suite of visualization tools called TensorBoard. ”\n",
    "\n",
    "-------------------\n",
    "### Remind Me About Tensorflow Again?\n",
    "\n",
    "- TensorFlow programs can range from a very simple to super complex problems (using thousands of computations), and they all have two basic components, **Operations**, and **Tensors**\n",
    "    - As explained in the other tutorials, the idea is that you create a model that consists of a set of operations\n",
    "        - We feed the data into the model and the tensors will flow between the operations until you get an output tensor\n",
    "            \n",
    "------------------\n",
    "### In Short...\n",
    "\n",
    "- TensorBoard provides us with a suite of web applications that help us to inspect and understand the TensorFlow runs and graphs \n",
    "    - Currently, it provides **five types of visualizations**: **scalars**, **images**, **audio**, **histograms**, and **graphs**\n",
    "-----------\n",
    "***When fully configured, TensorBoard window will look something like:***\n",
    "![Typical_Tensorboard_Window](inline_images/typical_tensorboard.png)\n",
    "     \n",
    "                                             Fig. 1. TensorBoard Typical Appearance\n",
    "  \n",
    "-----------\n",
    "### To What End!?  \n",
    "\n",
    "- TensorBoard is generally utilized for two main purposes:\n",
    "\n",
    "    1. Visualizing the Graph\n",
    "\n",
    "    2. Writing Summaries to Visualize Learning\n",
    "\n",
    "---------------\n",
    "**NOTE:**\n",
    "        \n",
    "        --> Learning to use TensorBoard early and often will make working with TensorFlow more enjoyable and productive\n",
    "        \n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualizing The Graph\n",
    "\n",
    "-------------------------------------\n",
    "While powerful, TensorFlow computation graphs can become extremely complicated. Visualizing the graph can help us understand and debug it. \n",
    "\n",
    "--------------------------------------\n",
    "***Here’s an example of the visualization at work from TensorFlow website***\n",
    "![tensor_board_graph_viz](inline_images/graph_viz.gif)\n",
    "     \n",
    "                         Fig. 2. Visualization of a TensorFlow graph   (Source: TensorFlow website)\n",
    "  \n",
    "-------------------------------------\n",
    "\n",
    "### How Do We Use Tensorboard in Tensorflow??  \n",
    "\n",
    "To make our TensorFlow program TensorBoard-activated, we need to add some lines of code\n",
    "- This will export the TensorFlow operations into a file, called event file (or event log file)\n",
    "- TensorBoard is able to read this file and give some insights of the model graph and its performance\n",
    "\n",
    "-------------------------------------\n",
    "**Now let’s write a simple TensorFlow program and visualize its computational graph with TensorBoard !!**\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "**NOTE ON TITLE NOMENCLATURE FORMAT**\n",
    "\n",
    "    --- example#.step#.substep#(if neccessary) \n",
    "    --- i.e. 1.2.2 is example 1, step 2, substep 2\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "**1.1 - Importing Required Libraries**\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "**1.2 - Creating A Graph**\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a graph that is the addition of two tensors\n",
    "\n",
    "a = tf.constant(2, name='a')\n",
    "b = tf.constant(3, name='b')\n",
    "\n",
    "c = tf.add(a, b, name='addition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "**1.3 - Running a Session**\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "**1.4.0 - Write An Event File**\n",
    "\n",
    "------------------\n",
    "\n",
    "- To visualize the program with TensorBoard, we need to write log files of the program\n",
    "- To write event files, we first need to create a writer for those logs using the following code\n",
    "\n",
    "======================================================================================================================\n",
    "                            \n",
    "                                >>> writer = tf.summary.FileWriter([logdir], [graph])\n",
    "                                \n",
    "                Where:\n",
    "                  |                                                                                           |\n",
    "                  |  -- [logdir]  is the folder where we want to store those log files                        |\n",
    "                  |           --> We can also choose [logdir] to be something meaningful such as ‘./graphs’   |\n",
    "                  |                                                                                           |\n",
    "                  |  -- [graph]  is the graph of the program we’re working on                                 |\n",
    "                            \n",
    "======================================================================================================================\n",
    "\n",
    "- This command can be executed in one of two ways depending on the handling of the graphing call\n",
    "    \n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "**1.4.1 - Write An Event File -- Method 1 -- Within A Session**\n",
    "\n",
    "------------------\n",
    "\n",
    "- Call the graph using sess.graph which returns the session’s graph (note that this requires us to have a session created)\n",
    "- This way is more common\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "**1.4.2 - Write An Event File -- Method 2 -- Outside A Session**\n",
    "\n",
    "------------------\n",
    "\n",
    "- Call the graph using tf.get_default_graph(), which returns the default graph of the program (not necessarily inside session)\n",
    "- This way is less common\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# creating the writer out of the session\n",
    "writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "\n",
    "# launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "***We can now see that two event files have been written into the graphs folder***\n",
    "![events_written](inline_images/event_snip.PNG)\n",
    "     \n",
    "                         Fig. 3. Directory Created (Titled Graphs) Containing Event Files\n",
    "  \n",
    "-------------------------------------\n",
    "*Note: In future use, **NEVER** have more than one event file in the graphs folder as it can cause tensorboard to get confused* <br>\n",
    "<br>\n",
    "*You should ensure you only have one in the folder at this point by deleting one we just created!*\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "**1.5.0 - Access Tensorboard Page**\n",
    "\n",
    "------------------\n",
    "\n",
    "- To visualize the graph we will be accessing a tensorboard session hosted on our computer in a similar way to the jupyter notebook\n",
    "- There are a few steps required\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "**1.5.1 - Access Tensorboard Page -- Open A Terminal Window**\n",
    "\n",
    "---------------\n",
    "\n",
    "- Any bash window (i.e. gitbash)\n",
    "\n",
    "---------------\n",
    "**1.5.2 - Access Tensorboard Page -- SSH Into The AWS EC2 Instance (If Required)**\n",
    "\n",
    "---------------\n",
    "\n",
    "![step_1](inline_images/step_1.PNG)\n",
    "     \n",
    "                              Fig. 4. Command To SSH Into The AWS EC2 Instance\n",
    "  \n",
    "---------------\n",
    "**1.5.3 - Access Tensorboard Page -- Activate The Environment You Wish To Use (command synatx may differ)**\n",
    "\n",
    "---------------\n",
    "\n",
    "![step_2](inline_images/step_2.PNG)\n",
    "     \n",
    "                     Fig. 5. Command To Activate The Tensorflow-Python3.6 Environment (Anaconda)\n",
    "  \n",
    "---------------\n",
    "**1.5.4 - Access Tensorboard Page -- Navigate To the Directory Containing the IPython Notebook**\n",
    "\n",
    "---------------\n",
    "\n",
    "![step_3](inline_images/step_3.PNG)\n",
    "     \n",
    "                      Fig. 6. Commands to Navigate To Directory Containing IPython Notebook (cd)\n",
    "  \n",
    "---------------\n",
    "**1.5.5 - Access Tensorboard Page -- Use the command  :  tensorboard --logdir=\"./graphs\" --port 6006**\n",
    "\n",
    "---------------\n",
    "\n",
    "![step_4](inline_images/step_4.PNG)\n",
    "     \n",
    "                                      Fig. 7. Command to Activate Tensorboard\n",
    "  \n",
    "---------------\n",
    "**1.5.6 - Access Tensorboard Page -- Find DNS Information From AWS EC2 Management Console**\n",
    "\n",
    "---------------\n",
    "\n",
    "![step_5](inline_images/step_5.PNG)\n",
    "     \n",
    "                         Fig. 8. Information Required To Access Tensorboard On Hosted Computer\n",
    "  \n",
    "---------------\n",
    "**1.5.7 - Access Tensorboard Page -- Enter The DNS Information Prefaced With *'http://'* and suffixed with *':6006'***\n",
    "\n",
    "---------------\n",
    "\n",
    "![step_6](inline_images/step_6.PNG)\n",
    "     \n",
    "                 Fig.9. URL Required to Access Tensorboard On Hosted Computer Including Prefix and Suffix\n",
    "  \n",
    "---------------\n",
    "**1.5.8 - Access Tensorboard Page -- You've Arrived!**\n",
    "\n",
    "---------------\n",
    "\n",
    "![step_7](inline_images/step_7.PNG)\n",
    "     \n",
    "                         Fig. 10. What The Tensorboard Window Will Look Like When You Arrive\n",
    "  \n",
    "---------------\n",
    "\n",
    "**Note:** \n",
    "- If we run our code several times with the same [logdir], multiple event files will be generated in our [logdir]\n",
    "- TF will only show the latest version of the graph and display the warning of multiple event files\n",
    "- The warning can be removed by deleting the event files that we no longer need or else we can save them in a different [logdir] folder\n",
    "- If additional problems are encountered, restart the kernel (and clear output), clear the log files, and re-run the required cells\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## 2. Writing Summaries To Visualize Learning\n",
    "\n",
    "-------------------------------------\n",
    "So far we only focused on how to visualize the graph in TensorBoard. Remember the other types of visualizations mentioned in the earlier part of the post that TensorBoard provides (**scalars**, **images** and **histograms**). In this part, we are going to use a special operation called ***summary*** to visualize the model parameters (like weights and biases of a neural network), metrics (like loss or accuracy value), and images (like input images to a network).\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "### What does the *Summary*  command do and how does it work ??\n",
    "\n",
    "- Summary is a special operation for TensorBoard that takes in a regular tensor and outputs the summarized data to your disk \n",
    "    - The data on the disk is stored in the event file discussed earlier  \n",
    "<br>\n",
    "- Basically, there are three main types of summaries:\n",
    "    1. **tf.summary.scalar**: Write a single scalar-valued tensor (like a classificaion loss or accuracy value)\n",
    "    2. **tf.summary.histogram**: Plot histogram of the values of a non-scalar tensor (used to visualize weight/bias matrices of a network)\n",
    "    3. **tf.summary.image**: Plot images (like input images of a network, or generated output images of an autoencoder or a GAN)\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "**In the following sections, let’s go through each of the above mentioned summary types in more detail!!**\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "**2.1.0 - tf.summary.scalar**\n",
    "\n",
    "-------------------\n",
    "- tf.summary.scalar is for writing the values of a scalar tensor that changes over time or iterations\n",
    "    - In the case of neural networks (say a simple network for classification task), it’s usually used to monitor the changes of loss function or classification accuracy\n",
    "\n",
    "--------------------\n",
    "**Let’s run a simple example to understand the point**\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "**2.1.1 - tf.summary.scalar -- Example**\n",
    "\n",
    "-------------------\n",
    "- Randomly pick 100 values from a standard Normal distribution, N(0, 1), and plot them one after the other.\n",
    "    - One way to do so is to simply create a variable and initialize it from a normal distribution (with mean=0 and std=1)\n",
    "        - Then we run a for-loop in the session and initialize it 100 times\n",
    "  \n",
    "  \n",
    "The code will be as follows and the required steps to write the summary is explained in the code:\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clear the defined variables and operations of all the previous cells\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 -- Create the Scalar Variable\n",
    "x_scalar = tf.get_variable('x_scalar', shape=[], initializer=tf.truncated_normal_initializer(mean=0, stddev=1))\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                  >>> tf.get_variable\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - Gets an existing variable with the defined parameters or create a new one\n",
    "#   - You can also use initializer\n",
    "#   - Recommended over tf.Variable for multiple reasons (slightly newer version of the same thing)\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                    >>> shape=[]\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - A constant has the shape [], this is simply the nomenclature\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                              >>> initializer = tf.truncated_normal_initializer(mean=0, stddev=1)\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - Initializer that generates a truncated normal distribution similar to random_normal_initializer\n",
    "#        -- Only difference is that values more than two standard deviations from the mean are discarded and re-drawn (truncated)\n",
    "#   - The mean of the distrubution is supplied as 0\n",
    "#   - The standard deviation of the distrubution is supplied as 1\n",
    "#        -- Therefore values smaller than -2 or larger than 2 are discared and redrawn\n",
    "#   - This is the recommended initializer for neural network weights and filters\n",
    "# ----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 -- Create the Scalar Summary\n",
    "scalar_summary = tf.summary.scalar(name='Scalar_Summary', tensor=x_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing the scalar summary\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Launch the Graph in a Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Step 4 - Creating the Writer Inside the Session\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    \n",
    "    # Step 5 - Define the Number of Iterations We Want to Preform\n",
    "    iters = 100\n",
    "    \n",
    "    # Step 6 - Loop through the defined number of iterations\n",
    "    for iteration in range(iters):\n",
    "        \n",
    "        # Step 7 - For each iteration, reinitialize the session (should pull a new constant from the distribution scalar_summary)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Step 8 - Evaluate the Scalar Summary (Since we reinitialized above, the value will be different than previous iterations)\n",
    "        summary = sess.run(scalar_summary)\n",
    "        \n",
    "        # Step 9 - Add the Summary to the Writer (i.e. to the event file) at a given iteration number\n",
    "        writer.add_summary(summary, iteration)\n",
    "    \n",
    "    \n",
    "    # Step 10 - CLOSE THE WRITER!!!! ( And Print Something When It's ALl Done )\n",
    "    writer.close()\n",
    "    \n",
    "    print('Done with writing the scalar summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "**Let’s pull up TensorBoard and Check the Result**  \n",
    "\n",
    "------------------\n",
    "Like before, you need to open terminal and type:\n",
    "\n",
    "                                     >>> tensorboard — logdir=”./graphs” — port 6006\n",
    "\n",
    "Here “./graphs” is the name of the directory we saved the event file to  \n",
    "  \n",
    "In TensorBoard, we find a new tab named “scalars” next to the “graphs” tab earlier discussed\n",
    "\n",
    "-----------------\n",
    "\n",
    "***The whole window should look similar to this:***\n",
    "![scalar_summary_tensorboard](inline_images/scalar_summary_tb.PNG)\n",
    "     \n",
    "                         Fig. 11. Tensorboard Example Recording 100 Nomrally Distributed Values \n",
    "  \n",
    "- In the figure, the plot panel is under the name “Scalar_Summary”; the same name that we defined in our code\n",
    "- The x-axis and y-axis respectively show the 100 steps (x) and the corresponding values for y (random values from a truncated normal distribution)\n",
    "\n",
    "-----------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "**2.2.0 - tf.summary.histogram**\n",
    "\n",
    "-------------------\n",
    "- tf.summary.histogram comes in handy if we want to observe the change of a value over time or iterations\n",
    "- It’s used for plotting the histogram of the values of a non-scalar tensor\n",
    "    - This provides us a view of how the histogram (and the distribution) of the tensor values change over time or iterations \n",
    "- In the case of neural networks, it’s commonly used to monitor the changes of weight and biase distributions\n",
    "    - It’s very useful in detecting irregular behavior of the network parameters \n",
    "        - i.e. vanishing/exploding weights and gradients\n",
    "\n",
    "--------------------\n",
    "**Now let’s go back to our previous example and add the histogram summary to it**\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "**2.2.1 - tf.summary.histogram -- Example**\n",
    "\n",
    "-------------------\n",
    "- We'll expand on the previous example by adding a matrix of size 30x40 whose entries come from a standard normal distribution\n",
    "    - We'll initialize this matrix 100 times and plot the distribution of its entries over time\n",
    "<br>\n",
    "<br>\n",
    "***NOTE:*** *Since our code will be embedded, it makes more sense just to start from scratch... in other words, much of this code will repeat what was used above as we are only adding in a few lines*\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clear the defined variables and operations of all the previous cells\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 -- Create the Scalar Variable\n",
    "x_scalar = tf.get_variable('x_scalar', shape=[], initializer=tf.truncated_normal_initializer(mean=0, stddev=1))\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                  >>> tf.get_variable\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - Gets an existing variable with the defined parameters or create a new one\n",
    "#   - You can also use initializer\n",
    "#   - Recommended over tf.Variable for multiple reasons (slightly newer version of the same thing)\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                    >>> shape=[]\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - A constant has the shape [], this is simply the nomenclature\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                              >>> initializer = tf.truncated_normal_initializer(mean=0, stddev=1)\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - Initializer that generates a truncated normal distribution similar to random_normal_initializer\n",
    "#        -- Only difference is that values more than two standard deviations from the mean are discarded and re-drawn (truncated)\n",
    "#   - The mean of the distrubution is supplied as 0\n",
    "#   - The standard deviation of the distrubution is supplied as 1\n",
    "#        -- Therefore values smaller than -2 or larger than 2 are discared and redrawn\n",
    "#   - This is the recommended initializer for neural network weights and filters\n",
    "# ----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 -- Create the Matrix Variable\n",
    "x_matrix = tf.get_variable('x_matrix', shape=[30, 40], initializer=tf.truncated_normal_initializer(mean=0, stddev=1))\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                  >>> tf.get_variable\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - Gets an existing variable with the defined parameters or create a new one\n",
    "#   - You can also use initializer\n",
    "#   - Recommended over tf.Variable for multiple reasons (slightly newer version of the same thing)\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                 >>> shape=[30, 40]\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - A matrix has the shape [rows, cols], this is simply the nomenclature\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                              >>> initializer = tf.truncated_normal_initializer(mean=0, stddev=1)\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - Initializer that generates a truncated normal distribution similar to random_normal_initializer\n",
    "#        -- Only difference is that values more than two standard deviations from the mean are discarded and re-drawn (truncated)\n",
    "#   - The mean of the distrubution is supplied as 0\n",
    "#   - The standard deviation of the distrubution is supplied as 1\n",
    "#        -- Therefore values smaller than -2 or larger than 2 are discared and redrawn\n",
    "#   - This is the recommended initializer for neural network weights and filters\n",
    "# ----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 -- Create the Scalar Summary\n",
    "scalar_summary = tf.summary.scalar(name='Scalar_Summary', tensor=x_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 -- Create the Histogram Summary\n",
    "histogram_summary = tf.summary.histogram(name='Histogram_Summary', values=x_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing the summaries\n"
     ]
    }
   ],
   "source": [
    "# Step 5 - Launch the Graph in a Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Step 6 - Creating the Writer Inside the Session\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    \n",
    "    # Step 7 - Define the Number of Iterations We Want to Preform\n",
    "    iters = 100\n",
    "    \n",
    "    # Step 8 - Loop through the defined number of iterations\n",
    "    for iteration in range(iters):\n",
    "        \n",
    "        # Step 9 - For each iteration, reinitialize the session (should pull a new constant from the distributions)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Step 10 - Evaluate the both the scalar and histogram summary\n",
    "        summary1, summary2 = sess.run([scalar_summary, histogram_summary])\n",
    "        \n",
    "        # Step 11 - Add Summary1 and Summary2 to the Writer (i.e. to the event file) for a given iteration number\n",
    "        writer.add_summary(summary1, iteration)\n",
    "        writer.add_summary(summary2, iteration)\n",
    "    \n",
    "    \n",
    "    # Step 12 - CLOSE THE WRITER!!!! ( And Print Something When It's ALl Done )\n",
    "    writer.close()\n",
    "    \n",
    "    print('Done with writing the summaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "In TensorBoard, two new tabs are added to the top menu: “Distributions” and “Histograms”\n",
    "\n",
    "------------\n",
    "***The results will look similar to the following***\n",
    "![histogram_summary_tensorboard](inline_images/histogram_summary_tb.png)\n",
    "     \n",
    "    Fig. 12. (a) scalar summary ------- (b) distribution ------- (c) histogram of the values of the 2D-tensor over 100 steps \n",
    "  \n",
    "------------\n",
    "\n",
    "In the figure, the “Distributions” tab contains a plot that shows the distribution of the values of the tensor (y-axis) through steps (x-axis). **You might ask what are the light and dark colors??**\n",
    "\n",
    "------------\n",
    "\n",
    "- The answer is that each line on the chart represents a percentile in the distribution over the data\n",
    "    - For example\n",
    "        - the bottom line (the very light one) shows how the minimum value has changed over time\n",
    "        - the line in the middle shows how the median has changed\n",
    "    - Reading from **top to bottom**, the lines have the following meaning:  **[maximum, 93%, 84%, 69%, 50%, 31%, 16%, 7%, minimum]**\n",
    "- These percentiles can also be viewed as standard deviation boundaries on a normal distribution\n",
    "    - Reading from **top to bottom**, the lines have the following meaning:  **[maximum, μ+1.5σ, μ+σ, μ+0.5σ, μ, μ-0.5σ, μ-σ, μ-1.5σ, minimum]** \n",
    "        - So that the colored regions, read from **inside to outside**, have widths **[σ, 2σ, 3σ]** respectively.\n",
    "\n",
    "-------------\n",
    "\n",
    "**What About the Histogram Panel??**\n",
    "\n",
    "------------\n",
    "\n",
    "In the histogram panel, each chart shows temporal “slices” of data\n",
    "- Each slice is a histogram of the tensor at a given step\n",
    "    - It’s organized with the oldest timestep in the back, and the most recent timestep in front\n",
    "- You can easily monitor the values on the histograms at any step\n",
    "    - Just move your cursor on the plot and see the x-y values on the histograms (*Fig. 13. (a)*)\n",
    "    - You can also change the Histogram Mode from “offset” to “overlay” (see *Fig. 13. (b)*) to see the histograms overlaid with one another\n",
    "    \n",
    "-------------\n",
    "\n",
    "![Histogram_Summary_2](inline_images/histogram_summary_tb2.png)\n",
    "  \n",
    "    Fig. 13. (a) monitor values on the histograms        --------        (b) overlayed histograms\n",
    "\n",
    "-------------\n",
    "\n",
    "As mentioned in the code, we need to run every summary (e.g. sess.run([scalar_summary, histogram_summary])) and then use our writer to write each of them to the disk. In practice, we can use any number of summaries to track different parameters in our model. **This makes running and writing the summaries extremly inefficient**.\n",
    "\n",
    "---------------\n",
    "The way around it is to merge all summaries in our graph and run them at once inside your session\n",
    "        \n",
    "        - This can be done with:\n",
    "                                          >>> tf.summary.merge_all()\n",
    "--------------                                        \n",
    "**Let’s add it to the Example and see how the code changes!!**\n",
    "<br>\n",
    "<br>\n",
    "***NOTE:*** *Since our code will be embedded, it makes more sense just to start from scratch... in other words, much of this code will repeat what was used above as we are only adding in a few lines*\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clear the defined variables and operations of all the previous cells\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 -- Create the Scalar Variable\n",
    "x_scalar = tf.get_variable('x_scalar', shape=[], initializer=tf.truncated_normal_initializer(mean=0, stddev=1))\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                  >>> tf.get_variable\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - Gets an existing variable with the defined parameters or create a new one\n",
    "#   - You can also use initializer\n",
    "#   - Recommended over tf.Variable for multiple reasons (slightly newer version of the same thing)\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                    >>> shape=[]\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - A constant has the shape [], this is simply the nomenclature\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                              >>> initializer = tf.truncated_normal_initializer(mean=0, stddev=1)\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - Initializer that generates a truncated normal distribution similar to random_normal_initializer\n",
    "#        -- Only difference is that values more than two standard deviations from the mean are discarded and re-drawn (truncated)\n",
    "#   - The mean of the distrubution is supplied as 0\n",
    "#   - The standard deviation of the distrubution is supplied as 1\n",
    "#        -- Therefore values smaller than -2 or larger than 2 are discared and redrawn\n",
    "#   - This is the recommended initializer for neural network weights and filters\n",
    "# ----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 -- Create the Matrix Variable\n",
    "x_matrix = tf.get_variable('x_matrix', shape=[30, 40], initializer=tf.truncated_normal_initializer(mean=0, stddev=1))\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                  >>> tf.get_variable\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - Gets an existing variable with the defined parameters or create a new one\n",
    "#   - You can also use initializer\n",
    "#   - Recommended over tf.Variable for multiple reasons (slightly newer version of the same thing)\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                 >>> shape=[30, 40]\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - A matrix has the shape [rows, cols], this is simply the nomenclature\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                              >>> initializer = tf.truncated_normal_initializer(mean=0, stddev=1)\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - Initializer that generates a truncated normal distribution similar to random_normal_initializer\n",
    "#        -- Only difference is that values more than two standard deviations from the mean are discarded and re-drawn (truncated)\n",
    "#   - The mean of the distrubution is supplied as 0\n",
    "#   - The standard deviation of the distrubution is supplied as 1\n",
    "#        -- Therefore values smaller than -2 or larger than 2 are discared and redrawn\n",
    "#   - This is the recommended initializer for neural network weights and filters\n",
    "# ----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 -- Create the Scalar Summary\n",
    "scalar_summary = tf.summary.scalar(name='Scalar_Summary', tensor=x_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 -- Create the Histogram Summary\n",
    "histogram_summary = tf.summary.histogram(name='Histogram_Summary', values=x_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 -- Merge The Summaries\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing the summaries\n"
     ]
    }
   ],
   "source": [
    "# Step 5 - Launch the Graph in a Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Step 6 - Creating the Writer Inside the Session\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    \n",
    "    # Step 7 - Define the Number of Iterations We Want to Preform\n",
    "    iters = 100\n",
    "    \n",
    "    # Step 8 - Loop through the defined number of iterations\n",
    "    for iteration in range(iters):\n",
    "        \n",
    "        # Step 9 - For each iteration, reinitialize the session (should pull a new constant from the distributions)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Step 10 - Evaluate the both the scalar and histogram summary as one in merged\n",
    "        summary = sess.run(merged)\n",
    "        \n",
    "        # Step 11 - Add Summary to the Writer (i.e. to the event file) for a given iteration number\n",
    "        writer.add_summary(summary, iteration)\n",
    "    \n",
    "    # Step 12 - CLOSE THE WRITER!!!! ( And Print Something When It's ALl Done )\n",
    "    writer.close()\n",
    "    \n",
    "    print('Done with writing the summaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "**2.3.0 - tf.summary.image**\n",
    "\n",
    "-------------------\n",
    "- As the name implies, this type of summary is used for writing and visualizing tensors as images\n",
    "    - In the case of neural networks, this is usually for either: \n",
    "        1. Tracking/Visualizing the images that are either fed to the network (say in each batch)\n",
    "        2. Tracking/Visualizing the images generated in the output (such as the reconstructed images in an autoencoder; or the fake images made by the generator model of a GAN)\n",
    "    - However, in general, this can be used for plotting any tensor\n",
    "        - For example, we can visualize a weight matrix of size 30x40 as an image of 30x40 pixels\n",
    "\n",
    "--------------------\n",
    "**An image summary can be created using:**\n",
    "\n",
    "======================================================================================================================\n",
    "                            \n",
    "                                >>> tf.summary.image(name, tensor, max_outputs=3)\n",
    "                                \n",
    "                Where:\n",
    "                  |                                                                                           |\n",
    "                  |  -- name         : is the name for the generated node (i.e. operation)                    |\n",
    "                  |  -- tensor       : is the desired tensor to be written as an image summary                |\n",
    "                  |  -- max_outputs  : is the maximum number of elements from tensor to generate images for   |\n",
    "\n",
    "======================================================================================================================\n",
    "\n",
    "------------------\n",
    "***FURTHER NOTE ON TENSOR SHAPE***  \n",
    "\n",
    "The tensor that we feed to tf.summary.image must be a 4-D tensor of shape **[batch_size, height, width, channels]**\n",
    "- Where batch_size is the number of images in the batch\n",
    "- The height and width determine the size of the image\n",
    "- The channels are: \n",
    "    - 1: for Grayscale images\n",
    "    - 3: for RGB (i.e. color) images\n",
    "    - 4: for RGBA images (where A stands for alpha; see RGBA)\n",
    "\n",
    "--------------------\n",
    "\n",
    "**Let's Look At An Example To Get a Better Understanding**\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "**2.3.1 - tf.summary.histogram -- Example**\n",
    "\n",
    "-------------------\n",
    "- Let’s define two variables and plot them as images in TensorBoard\n",
    "    1. Image Of size 30x10 as 3 grayscale images of size 10x10\n",
    "    2. Image Of size 50x30 as 5 color images of size 10x10\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clear the defined variables and operations of all the previous cells\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 -- Create the Variables\n",
    "w_gs = tf.get_variable('W_Grayscale', shape=[30, 10], initializer=tf.truncated_normal_initializer(mean=0, stddev=1))\n",
    "w_c = tf.get_variable('W_Color', shape=[50, 30], initializer=tf.truncated_normal_initializer(mean=0, stddev=1))\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                  >>> tf.get_variable\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - Gets an existing variable with the defined parameters or create a new one\n",
    "#   - You can also use initializer\n",
    "#   - Recommended over tf.Variable for multiple reasons (slightly newer version of the same thing)\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                    >>> shape=[##, ##]\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - A matrix has some inherent shape i.e. [10,30] or [50,30], this is simply the nomenclature\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#                              >>> initializer = tf.truncated_normal_initializer(mean=0, stddev=1)\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "#   - Initializer that generates a truncated normal distribution similar to random_normal_initializer\n",
    "#        -- Only difference is that values more than two standard deviations from the mean are discarded and re-drawn (truncated)\n",
    "#   - The mean of the distrubution is supplied as 0\n",
    "#   - The standard deviation of the distrubution is supplied as 1\n",
    "#        -- Therefore values smaller than -2 or larger than 2 are discared and redrawn\n",
    "#   - This is the recommended initializer for neural network weights and filters\n",
    "# ----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Reshape Our Data Into 4D-Tensors\n",
    "\n",
    "w_gs_reshaped = tf.reshape(w_gs, (3, 10, 10, 1))\n",
    "w_c_reshaped = tf.reshape(w_c, (5, 10, 10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create The Summaries and Merge Them\n",
    "\n",
    "gs_summary = tf.summary.image('Grayscale', w_gs_reshaped, max_outputs=3)\n",
    "c_summary = tf.summary.image('Color', w_c_reshaped, max_outputs=5)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with writing the summaries\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Launch the Graph in a Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Step 5 - Creating the Writer Inside the Session\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    \n",
    "    # Step 6 - Define the Number of Iterations We Want to Preform\n",
    "    iters = 100\n",
    "    \n",
    "    # Step 7 - Initialize the variables in the session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Step 8 - Evaluate the both the images to a summary\n",
    "    summary = sess.run(merged)\n",
    "\n",
    "    # Step 9 - Add Summary to the Writer (i.e. to the event file)\n",
    "    writer.add_summary(summary)\n",
    "    \n",
    "    # Step 10 - CLOSE THE WRITER!!!! ( And Print Something When It's ALl Done )\n",
    "    writer.close()\n",
    "    \n",
    "    print('Done with writing the summaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "***Now open TensorBoard like before and switch to IMAGES tab. The images should be something similar to:***\n",
    "![image_summary](inline_images/image_summary_tb.png)\n",
    "  \n",
    "    Fig. 13. (a) monitor values on the histograms        --------        (b) overlayed histograms\n",
    "\n",
    "-------------\n",
    "\n",
    "***NOTE: *** *We can similarly add any other image of any size to our summaries and plot them in TensorBoard*\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
