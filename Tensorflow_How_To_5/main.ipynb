{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "# Note: Once you enable eager execution, it cannot be disabled. \n",
    "tf.enable_eager_execution()\n",
    "\n",
    "# Import other libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"./data/Jordan_Peterson_Corpus.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2478554\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# unique contains all the unique characters in the file\n",
    "unique = sorted(set(text))\n",
    "\n",
    "print(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n', 0)\n",
      "(' ', 1)\n",
      "('!', 2)\n",
      "('\"', 3)\n",
      "('#', 4)\n",
      "('$', 5)\n",
      "('%', 6)\n",
      "('&', 7)\n",
      "(\"'\", 8)\n",
      "('(', 9)\n",
      "(')', 10)\n",
      "('*', 11)\n",
      "('+', 12)\n",
      "(',', 13)\n",
      "('-', 14)\n",
      "('.', 15)\n",
      "('/', 16)\n",
      "('0', 17)\n",
      "('1', 18)\n",
      "('2', 19)\n",
      "('3', 20)\n",
      "('4', 21)\n",
      "('5', 22)\n",
      "('6', 23)\n",
      "('7', 24)\n",
      "('8', 25)\n",
      "('9', 26)\n",
      "(':', 27)\n",
      "(';', 28)\n",
      "('<', 29)\n",
      "('=', 30)\n",
      "('?', 31)\n",
      "('A', 32)\n",
      "('B', 33)\n",
      "('C', 34)\n",
      "('D', 35)\n",
      "('E', 36)\n",
      "('F', 37)\n",
      "('G', 38)\n",
      "('H', 39)\n",
      "('I', 40)\n",
      "('J', 41)\n",
      "('K', 42)\n",
      "('L', 43)\n",
      "('M', 44)\n",
      "('N', 45)\n",
      "('O', 46)\n",
      "('P', 47)\n",
      "('Q', 48)\n",
      "('R', 49)\n",
      "('S', 50)\n",
      "('T', 51)\n",
      "('U', 52)\n",
      "('V', 53)\n",
      "('W', 54)\n",
      "('X', 55)\n",
      "('Y', 56)\n",
      "('Z', 57)\n",
      "('[', 58)\n",
      "(']', 59)\n",
      "('^', 60)\n",
      "('_', 61)\n",
      "('`', 62)\n",
      "('a', 63)\n",
      "('b', 64)\n",
      "('c', 65)\n",
      "('d', 66)\n",
      "('e', 67)\n",
      "('f', 68)\n",
      "('g', 69)\n",
      "('h', 70)\n",
      "('i', 71)\n",
      "('j', 72)\n",
      "('k', 73)\n",
      "('l', 74)\n",
      "('m', 75)\n",
      "('n', 76)\n",
      "('o', 77)\n",
      "('p', 78)\n",
      "('q', 79)\n",
      "('r', 80)\n",
      "('s', 81)\n",
      "('t', 82)\n",
      "('u', 83)\n",
      "('v', 84)\n",
      "('w', 85)\n",
      "('x', 86)\n",
      "('y', 87)\n",
      "('z', 88)\n",
      "('}', 89)\n",
      "('~', 90)\n"
     ]
    }
   ],
   "source": [
    "# creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(unique)}\n",
    "idx2char = {i:u for i, u in enumerate(unique)}\n",
    "\n",
    "for pair in zip(char2idx, idx2char):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the maximum length sentence we want for a single input in characters\n",
    "max_length = 100\n",
    "\n",
    "# length of the vocabulary in chars\n",
    "vocab_size = len(unique)\n",
    "\n",
    "# the embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# number of RNN (here GRU) units\n",
    "units = 1024\n",
    "\n",
    "# batch size \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# buffer size to shuffle our dataset\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24785, 100)\n",
      "(24785, 100)\n"
     ]
    }
   ],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "# Get's 100 words at a time where inps is 0-100 for instance targ will be 1-101\n",
    "for f in range(0, len(text)-max_length, max_length):\n",
    "    inps = text[f:f+max_length]\n",
    "    targ = text[f+1:f+1+max_length]\n",
    "\n",
    "    # Replace the text in the given chunks with the index of the appropriate charachter\n",
    "    input_text.append([char2idx[i] for i in inps])\n",
    "    target_text.append([char2idx[t] for t in targ])\n",
    "\n",
    "    \n",
    "print (np.array(input_text).shape)\n",
    "print (np.array(target_text).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------\n",
    "#            Dataset is a CLASS in Tensorflow (from_tensor_slices allows us to take slices)\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "#              * It creates a Dataset whose elements are slices of the given tensors *\n",
    "#\n",
    "# NOTE: If tensors contains a npy array, and eager execution is not enabled, the values will be embedded in the graph\n",
    "#\n",
    "# Arguments:\n",
    "#    tensors: A nested structure of tensors, each having the same size in the 0th dimension.\n",
    "#\n",
    "# Returns:\n",
    "#    Dataset: A Dataset\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create our dataset and shuffle accordingly (note that the input text and target text are what make up the dataset)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Break the dataset into batches (adds a dimension) and drop remiander if batches are not the proper size\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "    \n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.batch_sz = batch_size\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        if tf.test.is_gpu_available():\n",
    "            self.gru = tf.keras.layers.CuDNNGRU(self.units, \n",
    "                                              return_sequences=True, \n",
    "                                              return_state=True, \n",
    "                                              recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            self.gru = tf.keras.layers.GRU(self.units, \n",
    "                                         return_sequences=True, \n",
    "                                         return_state=True, \n",
    "                                         recurrent_activation='sigmoid', \n",
    "                                         recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # output shape == (batch_size, max_length, hidden_size) \n",
    "        # states shape == (batch_size, hidden_size)\n",
    "\n",
    "        # states variable to preserve the state of the model\n",
    "        # this will be used to pass at every step to the model while training\n",
    "        output, states = self.gru(x, initial_state=hidden)\n",
    "\n",
    "        # reshaping the output so that we can pass it to the Dense layer\n",
    "        # after reshaping the shape is (batch_size * max_length, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # The dense layer will output predictions for every time_steps(max_length)\n",
    "        # output shape after the dense layer == (max_length * batch_size, vocab_size)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(vocab_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
    "def loss_function(real, preds):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.9135\n",
      "Epoch 1 Batch 100 Loss 0.9669\n",
      "Epoch 1 Batch 200 Loss 0.9719\n",
      "Epoch 1 Batch 300 Loss 1.0085\n",
      "Epoch 1 Loss 0.9956\n",
      "Time taken for 1 epoch 15.225118398666382 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.8784\n",
      "Epoch 2 Batch 100 Loss 0.9487\n",
      "Epoch 2 Batch 200 Loss 0.9873\n",
      "Epoch 2 Batch 300 Loss 1.0212\n",
      "Epoch 2 Loss 1.0188\n",
      "Time taken for 1 epoch 15.268008708953857 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.9151\n",
      "Epoch 3 Batch 100 Loss 0.9587\n",
      "Epoch 3 Batch 200 Loss 0.9879\n",
      "Epoch 3 Batch 300 Loss 1.0265\n",
      "Epoch 3 Loss 1.0021\n",
      "Time taken for 1 epoch 15.327991724014282 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.9538\n",
      "Epoch 4 Batch 100 Loss 0.9298\n",
      "Epoch 4 Batch 200 Loss 0.9861\n",
      "Epoch 4 Batch 300 Loss 1.0195\n",
      "Epoch 4 Loss 1.0549\n",
      "Time taken for 1 epoch 15.275188684463501 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.9390\n",
      "Epoch 5 Batch 100 Loss 0.9764\n",
      "Epoch 5 Batch 200 Loss 1.0219\n",
      "Epoch 5 Batch 300 Loss 1.0369\n",
      "Epoch 5 Loss 1.0156\n",
      "Time taken for 1 epoch 15.68049669265747 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.9355\n",
      "Epoch 6 Batch 100 Loss 0.9953\n",
      "Epoch 6 Batch 200 Loss 0.9950\n",
      "Epoch 6 Batch 300 Loss 1.0470\n",
      "Epoch 6 Loss 1.0464\n",
      "Time taken for 1 epoch 15.328130960464478 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.9388\n",
      "Epoch 7 Batch 100 Loss 0.9883\n",
      "Epoch 7 Batch 200 Loss 1.0420\n",
      "Epoch 7 Batch 300 Loss 1.0217\n",
      "Epoch 7 Loss 1.0787\n",
      "Time taken for 1 epoch 15.345691680908203 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.0161\n",
      "Epoch 8 Batch 100 Loss 1.0043\n",
      "Epoch 8 Batch 200 Loss 1.0381\n",
      "Epoch 8 Batch 300 Loss 1.1004\n",
      "Epoch 8 Loss 1.0996\n",
      "Time taken for 1 epoch 15.40909194946289 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.0369\n",
      "Epoch 9 Batch 100 Loss 1.0026\n",
      "Epoch 9 Batch 200 Loss 0.9966\n",
      "Epoch 9 Batch 300 Loss 1.1003\n",
      "Epoch 9 Loss 1.1062\n",
      "Time taken for 1 epoch 15.341733932495117 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.9969\n",
      "Epoch 10 Batch 100 Loss 1.0475\n",
      "Epoch 10 Batch 200 Loss 1.0670\n",
      "Epoch 10 Batch 300 Loss 1.0916\n",
      "Epoch 10 Loss 1.0967\n",
      "Time taken for 1 epoch 15.575653314590454 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.9986\n",
      "Epoch 11 Batch 100 Loss 1.0101\n",
      "Epoch 11 Batch 200 Loss 1.0777\n",
      "Epoch 11 Batch 300 Loss 1.0877\n",
      "Epoch 11 Loss 1.1919\n",
      "Time taken for 1 epoch 15.488707542419434 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.0358\n",
      "Epoch 12 Batch 100 Loss 1.0255\n",
      "Epoch 12 Batch 200 Loss 1.1213\n",
      "Epoch 12 Batch 300 Loss 1.1850\n",
      "Epoch 12 Loss 1.2608\n",
      "Time taken for 1 epoch 15.352133989334106 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.1036\n",
      "Epoch 13 Batch 100 Loss 1.1626\n",
      "Epoch 13 Batch 200 Loss 1.1367\n",
      "Epoch 13 Batch 300 Loss 1.1796\n",
      "Epoch 13 Loss 1.2026\n",
      "Time taken for 1 epoch 15.480679035186768 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.1316\n",
      "Epoch 14 Batch 100 Loss 1.1062\n",
      "Epoch 14 Batch 200 Loss 1.1319\n",
      "Epoch 14 Batch 300 Loss 1.1813\n",
      "Epoch 14 Loss 1.3215\n",
      "Time taken for 1 epoch 15.442988634109497 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.1820\n",
      "Epoch 15 Batch 100 Loss 1.1725\n",
      "Epoch 15 Batch 200 Loss 1.2429\n",
      "Epoch 15 Batch 300 Loss 1.2146\n",
      "Epoch 15 Loss 1.3306\n",
      "Time taken for 1 epoch 15.580900192260742 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.1685\n",
      "Epoch 16 Batch 100 Loss 1.1969\n",
      "Epoch 16 Batch 200 Loss 1.2395\n",
      "Epoch 16 Batch 300 Loss 1.2281\n",
      "Epoch 16 Loss 1.2657\n",
      "Time taken for 1 epoch 15.41695237159729 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.2043\n",
      "Epoch 17 Batch 100 Loss 1.2077\n",
      "Epoch 17 Batch 200 Loss 1.1534\n",
      "Epoch 17 Batch 300 Loss 1.2254\n",
      "Epoch 17 Loss 1.2770\n",
      "Time taken for 1 epoch 15.429816246032715 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.1550\n",
      "Epoch 18 Batch 100 Loss 1.1789\n",
      "Epoch 18 Batch 200 Loss 1.2182\n",
      "Epoch 18 Batch 300 Loss 1.2958\n",
      "Epoch 18 Loss 1.2270\n",
      "Time taken for 1 epoch 15.409378051757812 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.2227\n",
      "Epoch 19 Batch 100 Loss 1.1854\n",
      "Epoch 19 Batch 200 Loss 1.2731\n",
      "Epoch 19 Batch 300 Loss 1.2628\n",
      "Epoch 19 Loss 1.3988\n",
      "Time taken for 1 epoch 15.43992805480957 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.3530\n",
      "Epoch 20 Batch 100 Loss 1.2974\n",
      "Epoch 20 Batch 200 Loss 1.3580\n",
      "Epoch 20 Batch 300 Loss 1.4004\n",
      "Epoch 20 Loss 1.4683\n",
      "Time taken for 1 epoch 15.786572694778442 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.3870\n",
      "Epoch 21 Batch 100 Loss 1.3860\n",
      "Epoch 21 Batch 200 Loss 1.3933\n",
      "Epoch 21 Batch 300 Loss 1.3821\n",
      "Epoch 21 Loss 1.3927\n",
      "Time taken for 1 epoch 15.539708137512207 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 1.2823\n",
      "Epoch 22 Batch 100 Loss 1.3567\n",
      "Epoch 22 Batch 200 Loss 1.4156\n",
      "Epoch 22 Batch 300 Loss 1.4533\n",
      "Epoch 22 Loss 1.4581\n",
      "Time taken for 1 epoch 15.441995620727539 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 1.3728\n",
      "Epoch 23 Batch 100 Loss 1.2562\n",
      "Epoch 23 Batch 200 Loss 1.3453\n",
      "Epoch 23 Batch 300 Loss 1.3440\n",
      "Epoch 23 Loss 1.4345\n",
      "Time taken for 1 epoch 15.355863809585571 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 1.3658\n",
      "Epoch 24 Batch 100 Loss 1.3653\n",
      "Epoch 24 Batch 200 Loss 1.3774\n",
      "Epoch 24 Batch 300 Loss 1.4537\n",
      "Epoch 24 Loss 1.5687\n",
      "Time taken for 1 epoch 15.408761501312256 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 1.4907\n",
      "Epoch 25 Batch 100 Loss 1.5677\n",
      "Epoch 25 Batch 200 Loss 1.9194\n",
      "Epoch 25 Batch 300 Loss 2.2465\n",
      "Epoch 25 Loss 2.0873\n",
      "Time taken for 1 epoch 15.578478574752808 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "\n",
    "EPOCHS = 25\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch, (inp, target)) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # feeding the hidden state back into the model\n",
    "            # This is the interesting step\n",
    "            predictions, hidden = model(inp, hidden)\n",
    "\n",
    "            # reshaping the target because that's how the \n",
    "            # loss function expects it\n",
    "            target = tf.reshape(target, (-1,))\n",
    "            loss = loss_function(target, predictions)\n",
    "              \n",
    "            grads = tape.gradient(loss, model.variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.variables))\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1, batch, loss))\n",
    "    \n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7fbf98153ac8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quence for the relize house and the constical und the \n",
      "constical and not the constions and the somethe the servers of the in ad some a precion and the \n",
      "come the is and \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation step(generating text using the model learned)\n",
    "\n",
    "# number of characters to generate\n",
    "num_generate = 1000\n",
    "\n",
    "# You can change the start string to experiment\n",
    "start_string = 'q'\n",
    "# converting our start string to numbers(vectorizing!) \n",
    "input_eval = [char2idx[s] for s in start_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "# empty string to store our results\n",
    "text_generated = ''\n",
    "\n",
    "# hidden state shape == (batch_size, number of rnn units); here batch size == 1\n",
    "hidden = [tf.zeros((1, units))]\n",
    "for i in range(num_generate):\n",
    "    predictions, hidden = model(input_eval, hidden)\n",
    "\n",
    "    # using argmax to predict the word returned by the model\n",
    "    predicted_id = tf.argmax(predictions[-1]).numpy()\n",
    "    \n",
    "    # We pass the predicted word as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    text_generated += idx2char[predicted_id]\n",
    "\n",
    "print (start_string + text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
