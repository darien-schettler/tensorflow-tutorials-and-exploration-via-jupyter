{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <center>Language Model for Sequence to Sequence Modelling\n",
    "***\n",
    "***\n",
    "## <center>English to French Translation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### You can separate the entire model into 2 Smaller Sub-Models\n",
    "\n",
    "1. The first sub-model is known as the **<font color=\"blue\">Encoder [E] Model</font>**\n",
    "2. The second sub-model is known as the **<font color=\"red\">Decoder [D] Model</font>**\n",
    "\n",
    "***\n",
    "- **<font color=\"blue\">[E]</font>** takes raw text data as the input\n",
    "    - This is similar to any other RNN architecture\n",
    "- **<font color=\"blue\">[E]</font>** returns a neural representation as the output\n",
    "    - This is similar to most RNNs however the output of this model is not the end of the line... it is simply the input for the next submodel... **<font color=\"red\">model [D]</font>**\n",
    "<br><br>\n",
    "- **<font color=\"red\">[D]</font>** takes the output of **<font color=\"blue\">[E]</font>** as its input (a vector embedding)\n",
    "    - This is unique and is the keystone principle of the encoder-decoder type of network\n",
    "- **<font color=\"red\">[D]</font>** returns text (usually a sentance)\n",
    "    - This text represents the net output of the model and allows us to go from text-in to text-out.\n",
    "        - In our case the text-in will be in *English* and the text-out will be in *French*\n",
    "***\n",
    "\n",
    "This starts to explain why we call **<font color=\"blue\">[E]</font>** the **<font color=\"blue\">Encoder</font>** and **<font color=\"red\">[D]</font>** the **<font color=\"red\">Decoder</font>**\n",
    "***\n",
    "- **<font color=\"blue\">[E]</font>** makes an output vector that contains a neurel embedding of some text\n",
    "    - From the outside looking in we don’t know what it really is\n",
    "        - It is somewhat encrypted\n",
    "- **<font color=\"red\">[D]</font>** has the ability to read **<font color=\"blue\">[E]’s</font>** encoding/output and make sense of it\n",
    "    - This allows  **<font color=\"blue\">[E]</font>** to create a totally new set of text as its output\n",
    "        - The translated French in this case\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inline1](./inline_images/inline_RNN_1.png)\n",
    "<center>* **Fig. 1.** Architecture For Neurel Machine Translation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Let's Define The Functions We Will Be Using Below:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_model_inputs():\n",
    "    # Define placeholders for inputs, targets and target_sequence_length (see previous tutorials for help on placeholders)\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # NOTE: Shape is set to None if we are unsure of what it will be at runtime\n",
    "    \n",
    "    # The inputs placeholder will hold the english sentances we wish to translate\n",
    "    inputs = tf.placeholder(dtype=tf.int32,        # dtype set to be a 32 bit integer\n",
    "                            shape=[None, None],    # shape is a two dimensional tensor -- [batch size, maximum sentance length]\n",
    "                            name='input')          # name is 'input'\n",
    "    \n",
    "    # The targets placeholder is similar but it will hold the output french sentances (post-translation)\n",
    "    targets = tf.placeholder(dtype=tf.int32,       # dtype set to be a 32 bit integer\n",
    "                             shape=[None, None],   # shape is a two dimensional tensor -- [batch size, maximum sentnace length]\n",
    "                             name='targets')       # name is 'targets'\n",
    "\n",
    "    # Represents the length of each output sentance\n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
    "    \n",
    "    return inputs, targets, target_sequence_length, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
